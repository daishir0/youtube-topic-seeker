#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ•ã‚§ãƒ¼ã‚º3ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰å‰Šé™¤ã—ã€
ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ç¢ºä¿ã—ã¾ã™ã€‚
"""

import json
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Set, Tuple
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent / "src"))

from config import Config
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
import httpx

class VectorStoreCleanup:
    """ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.config = Config()
        self.setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # OpenAI Embeddingsè¨­å®š
        http_client = None
        proxy_settings = self.config.get_proxy_settings()
        if proxy_settings:
            proxy_url = self.config.get_proxy_url()
            http_client = httpx.Client(proxy=proxy_url)
        
        self.embeddings = OpenAIEmbeddings(
            model=self.config.rag.embedding_model,
            openai_api_key=self.config.openai.api_key,
            http_client=http_client
        )
        
        self.vectorstore_dir = Path('data/vectorstore')
        self.phase1_dir = Path('data/1-plain')
        
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    
    def identify_orphaned_data(self) -> Tuple[Set[str], Set[str], Set[str]]:
        """å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®šã™ã‚‹"""
        self.logger.info("å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å®šã‚’é–‹å§‹...")
        
        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰å‡¦ç†æ¸ˆã¿å‹•ç”»IDã‚’å–å¾—
        build_info_file = self.vectorstore_dir / "build_info.json"
        if not build_info_file.exists():
            self.logger.error("build_info.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return set(), set(), set()
        
        with open(build_info_file, 'r') as f:
            build_info = json.load(f)
        
        vectorstore_video_ids = set(build_info.get('processed_video_ids', []))
        self.logger.info(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢å†…ã®å‹•ç”»æ•°: {len(vectorstore_video_ids)}")
        
        # ãƒ•ã‚§ãƒ¼ã‚º1ã«å­˜åœ¨ã™ã‚‹å‹•ç”»IDã‚’åé›†
        phase1_video_ids = set()
        cutoff_date = datetime.now() - timedelta(days=365)
        
        for metadata_file in self.phase1_dir.rglob('*/metadata.json'):
            try:
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                
                video_id = metadata.get('id')
                upload_date_str = metadata.get('upload_date', '')
                
                if video_id and upload_date_str:
                    # 1å¹´ä»¥å†…ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å¯¾è±¡
                    upload_date = datetime.strptime(upload_date_str, '%Y%m%d')
                    if upload_date >= cutoff_date:
                        phase1_video_ids.add(video_id)
                        
            except Exception as e:
                self.logger.warning(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {metadata_file}: {e}")
                continue
        
        self.logger.info(f"ãƒ•ã‚§ãƒ¼ã‚º1ã®æœ‰åŠ¹å‹•ç”»æ•°ï¼ˆ1å¹´ä»¥å†…ï¼‰: {len(phase1_video_ids)}")
        
        # å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ç‰¹å®š
        orphaned_video_ids = vectorstore_video_ids - phase1_video_ids
        valid_video_ids = vectorstore_video_ids & phase1_video_ids
        
        self.logger.info(f"å­¤ç«‹ãƒ‡ãƒ¼ã‚¿: {len(orphaned_video_ids)}æœ¬")
        self.logger.info(f"æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(valid_video_ids)}æœ¬")
        
        return orphaned_video_ids, valid_video_ids, vectorstore_video_ids
    
    def remove_orphaned_data_from_vectorstore(self, orphaned_video_ids: Set[str]) -> bool:
        """ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
        if not orphaned_video_ids:
            self.logger.info("å‰Šé™¤å¯¾è±¡ã®å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
            return True
        
        self.logger.info(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰{len(orphaned_video_ids)}æœ¬ã®å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤é–‹å§‹...")
        
        try:
            # Chromaãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ãƒ­ãƒ¼ãƒ‰
            vectorstore = Chroma(
                persist_directory=str(self.vectorstore_dir),
                embedding_function=self.embeddings
            )
            
            # å‰Šé™¤å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’ç‰¹å®š
            # Chromaã§ã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®Video_idã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            deleted_count = 0
            batch_size = 50  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦å®‰å…¨ã«å‡¦ç†
            
            orphaned_list = list(orphaned_video_ids)
            for i in range(0, len(orphaned_list), batch_size):
                batch_ids = orphaned_list[i:i + batch_size]
                
                try:
                    # è©²å½“ã™ã‚‹å‹•ç”»IDã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
                    for video_id in batch_ids:
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§æ¤œç´¢ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’å–å¾—
                        results = vectorstore.get(
                            where={"video_id": video_id}
                        )
                        
                        if results and results['ids']:
                            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤
                            vectorstore.delete(ids=results['ids'])
                            deleted_count += len(results['ids'])
                            self.logger.debug(f"å‰Šé™¤å®Œäº†: {video_id} ({len(results['ids'])}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ)")
                
                except Exception as e:
                    self.logger.error(f"ãƒãƒƒãƒå‰Šé™¤ã‚¨ãƒ©ãƒ¼ {batch_ids}: {e}")
                    continue
            
            self.logger.info(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰{deleted_count}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤å®Œäº†")
            return True
            
        except Exception as e:
            self.logger.error(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def update_build_info(self, valid_video_ids: Set[str]) -> bool:
        """build_info.jsonã‚’æ›´æ–°"""
        self.logger.info("build_info.jsonã‚’æ›´æ–°ä¸­...")
        
        try:
            build_info_file = self.vectorstore_dir / "build_info.json"
            
            with open(build_info_file, 'r') as f:
                build_info = json.load(f)
            
            # æœ‰åŠ¹ãªå‹•ç”»IDã®ã¿ã«æ›´æ–°
            build_info['processed_video_ids'] = list(valid_video_ids)
            build_info['total_videos'] = len(valid_video_ids)
            build_info['cleaned_at'] = datetime.now().isoformat()
            build_info['cleanup_removed_count'] = len(build_info.get('processed_video_ids', [])) - len(valid_video_ids)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_file = build_info_file.with_suffix('.json.backup')
            with open(backup_file, 'w') as f:
                json.dump(json.load(open(build_info_file)), f, indent=2, ensure_ascii=False)
            
            # æ›´æ–°ç‰ˆã‚’ä¿å­˜
            with open(build_info_file, 'w') as f:
                json.dump(build_info, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"build_info.jsonæ›´æ–°å®Œäº†ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}ï¼‰")
            return True
            
        except Exception as e:
            self.logger.error(f"build_info.jsonæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def verify_data_consistency(self, valid_video_ids: Set[str]) -> bool:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æœ€çµ‚ç¢ºèª"""
        self.logger.info("ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ç¢ºèªä¸­...")
        
        try:
            # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰å‹•ç”»IDã‚’å†å–å¾—
            vectorstore = Chroma(
                persist_directory=str(self.vectorstore_dir),
                embedding_function=self.embeddings
            )
            
            # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—
            all_docs = vectorstore.get()
            vectorstore_video_ids = set()
            
            for metadata in all_docs.get('metadatas', []):
                if metadata and 'video_id' in metadata:
                    vectorstore_video_ids.add(metadata['video_id'])
            
            # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            missing_in_vectorstore = valid_video_ids - vectorstore_video_ids
            extra_in_vectorstore = vectorstore_video_ids - valid_video_ids
            
            self.logger.info(f"æœ€çµ‚ç¢ºèªçµæœ:")
            self.logger.info(f"  - æœ‰åŠ¹å‹•ç”»IDæ•°: {len(valid_video_ids)}")
            self.logger.info(f"  - ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢å‹•ç”»IDæ•°: {len(vectorstore_video_ids)}")
            self.logger.info(f"  - ä¸è¶³ãƒ‡ãƒ¼ã‚¿: {len(missing_in_vectorstore)}")
            self.logger.info(f"  - ä½™å‰°ãƒ‡ãƒ¼ã‚¿: {len(extra_in_vectorstore)}")
            
            if missing_in_vectorstore:
                self.logger.warning(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ä¸è¶³: {list(missing_in_vectorstore)[:5]}...")
            
            if extra_in_vectorstore:
                self.logger.warning(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ä½™å‰°: {list(extra_in_vectorstore)[:5]}...")
            
            return len(extra_in_vectorstore) == 0
            
        except Exception as e:
            self.logger.error(f"æ•´åˆæ€§ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_cleanup(self) -> bool:
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ã‚’å®Ÿè¡Œ"""
        self.logger.info("=== ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹ ===")
        
        try:
            # 1. å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ç‰¹å®š
            orphaned_ids, valid_ids, original_ids = self.identify_orphaned_data()
            
            if not orphaned_ids:
                self.logger.info("å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸è¦ã§ã™ã€‚")
                return True
            
            # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            print(f"\nğŸ” ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡:")
            print(f"  ğŸ“Š ç¾åœ¨ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢å‹•ç”»æ•°: {len(original_ids)}")
            print(f"  ğŸŸ¢ æœ‰åŠ¹å‹•ç”»æ•°: {len(valid_ids)}")
            print(f"  ğŸ”´ å‰Šé™¤å¯¾è±¡ï¼ˆå­¤ç«‹ãƒ‡ãƒ¼ã‚¿ï¼‰: {len(orphaned_ids)}")
            print(f"\nâš ï¸  {len(orphaned_ids)}æœ¬ã®å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã™ã€‚")
            
            # è‡ªå‹•å®Ÿè¡Œã®ãŸã‚ç¢ºèªã‚’ã‚¹ã‚­ãƒƒãƒ—
            print("âœ… è‡ªå‹•å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™...")
            confirm = 'y'
            
            # 2. ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰å‰Šé™¤
            if not self.remove_orphaned_data_from_vectorstore(orphaned_ids):
                return False
            
            # 3. build_info.jsonæ›´æ–°
            if not self.update_build_info(valid_ids):
                return False
            
            # 4. æ•´åˆæ€§ç¢ºèª
            if not self.verify_data_consistency(valid_ids):
                self.logger.warning("æ•´åˆæ€§ç¢ºèªã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                return False
            
            self.logger.info("=== ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº† ===")
            print(f"\nâœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"ğŸ“Š æœ€çµ‚çµæœ: {len(valid_ids)}æœ¬ã®å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãŒä¿æŒã•ã‚Œã¦ã„ã¾ã™")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    cleanup = VectorStoreCleanup()
    success = cleanup.run_cleanup()
    
    if not success:
        print("âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    
    print("ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()